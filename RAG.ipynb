{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"langchain_395b1489de9e42f8ad53861adc6ff137_e1aniIhoxP7F\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:14-05:00\",\"took\":142865}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Shutting down... \",\"time\":\"2026-02-17T14:02:14-05:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Stopped serving weaviate at http://127.0.0.1:8079\",\"time\":\"2026-02-17T14:02:14-05:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running RAG Query ---\n",
      "Embedded weaviate wasn't listening on port 8079, so starting embedded weaviate again\n",
      "Started /home/skhalid/.cache/weaviate-embedded: process ID 604375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2026-02-17T14:02:15-05:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2026-02-17T14:02:15-05:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_0de99c329b794413afc2bd7fa2079745_0nSUlC86ztMb\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":267661}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_16d2307bad634d97ba624b458ad97114_wsMsfoMRSllA\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":271205}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_321d55a7b6814147b1941b6c598d9530_WLROJZgl3SBK\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":117759}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_395b1489de9e42f8ad53861adc6ff137_e1aniIhoxP7F\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":212887}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_3a358fe44e7a4210ae583a4fc48ca6f3_nWxJqZHaNyaw\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":88872}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_3ef2c8fc1b844fc79c580cb96559e798_QToHv1NjCCTS\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":118888}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_42c671aa6a164c5986e98f1e85f47c32_drTiqyJEscES\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":126432}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_4679259a327e4406ad258a8d0e7e983e_J1pyHwgHhfwJ\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":586593}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_4960a6d565db4f26be5e648c1f891d22_IEQ9KTEjrI8V\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":411283}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_51ea60b0bc3c438d926d09a14efa3a58_dizDcImloEBo\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":145181}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_57ddf17b098040e9aa57b3ad4f21133c_fI0xtkM0ncdW\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":212546}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_601896998b17449294e8bbfc1d02d49d_MqKht0WC9JM2\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":126098}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_6ae79fdc969e48288d52c777bb6bb7f2_4Av2ZXNhwDtw\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":231653}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_73d776ca835747f2a7194b4c317538d4_VG4DEJ60pSsP\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":987839}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_7a0777f208894d71b15fab5809e8b18a_QGVgTiT1484d\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":63556}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_7ab59d3351954ac887be4a5ca7b7ed75_gRThgYtaVmpz\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":908129}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_99c22b904a094576a7dcc639fb181e2b_vu8ZiFlXkbpC\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":1179794}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_938843b23407408ba3cfaf8d49ed985e_REsylxABdce1\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":4236291}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_9ea9a7011bf0445ba97466be0f069492_ijhKJMBm2m1I\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":185474}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_a83cb9987c7a4ecbb0385c49181876c6_GZqsaPesEVow\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":8151277}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_af8726664c484280a67aa0494989455f_eJtOZ087ONLH\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":1023703}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_ab95508327224012afe4e531a874dd9f_qLB9kdt3RXg9\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":7293243}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_bd8843da91a24c7d833d5380bb29e055_PDFjA7yTIF1Q\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":232920}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_bfd53e78b91c4b6aacd2c213460b3f86_hY3JDVA4X38l\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":1069758}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_c17489c84171418d96f7b2c04795212e_WvEf1OXNcVi0\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":232261}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_ca34d7dd78ae45bba2fe29f5560c1b25_6MV5xKCzswD6\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":236513}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_dd9167725bb944fbb892f1e969ff33b4_jCbaGsGWMxNz\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":232626}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_dea2ca38b84e43afb254c8154df1bbbe_7cNzganS0XhL\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":1092782}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"langchain_df8da075edc142dfbe6d9ba3d285aa54_kNb1XPHuTWQV\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2026-02-17T14:02:15-05:00\",\"took\":248709}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2026-02-17T14:02:15-05:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2026-02-17T14:02:15-05:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2026-02-17T14:02:15-05:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack fell down and broke his crown while going up the hill with Jill. After the fall, he got up and went home to rest and mend his head. He used vinegar and brown paper for his injury.\n",
      "\n",
      "--- Running another RAG Query ---\n",
      "Jill tumbled down the hill after Jack fell. The context does not specify any further details about her condition. Therefore, it is unclear what specifically happened to her.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List, TypedDict\n",
    "\n",
    "import dotenv\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "from typing import TypedDict, List\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "# --- 1. Data Preparation ---\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/SajeelKA/AgenticSamples/refs/heads/main/resource.txt'\n",
    "res = requests.get(url)\n",
    "\n",
    "with open(\"resource1.txt\", \"w\") as f:\n",
    "    f.write(res.text)\n",
    "\n",
    "loader = TextLoader(\"./resource1.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# --- 2. Setup Weaviate (Embedded) ---\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions()\n",
    ")\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    client=client,\n",
    "    by_text=False,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# --- 3. Initialize LLM ---\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # modern replacement for gpt-3.5-turbo\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "class RAGGraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "\n",
    "# --- 5. Nodes ---\n",
    "\n",
    "def retrieve_documents_node(state: RAGGraphState) -> RAGGraphState:\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": documents,\n",
    "        \"generation\": \"\"\n",
    "    }\n",
    "\n",
    "def generate_response_node(state: RAGGraphState) -> RAGGraphState:\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    template = \"\"\"You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, say that you don't know.\n",
    "                Use three sentences maximum and keep the answer concise.\n",
    "                \n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "                Answer:\n",
    "                \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    generation = rag_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": documents,\n",
    "        \"generation\": generation\n",
    "    }\n",
    "\n",
    "# --- 6. Build Graph ---\n",
    "\n",
    "workflow = StateGraph(RAGGraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_documents_node)\n",
    "workflow.add_node(\"generate\", generate_response_node)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- 7. Run App ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Running RAG Query ---\")\n",
    "    query = \"What happened to Jack?\"\n",
    "    inputs = {\"question\": query}\n",
    "\n",
    "    for s in app.stream(inputs):\n",
    "        if 'generate' in s.keys():\n",
    "            print(s['generate']['generation'])\n",
    "\n",
    "    print(\"\\n--- Running another RAG Query ---\")\n",
    "    query_2 = \"What happened to Jill?\"\n",
    "    inputs_2 = {\"question\": query_2}\n",
    "\n",
    "    for s in app.stream(inputs_2):\n",
    "        # print(s)\n",
    "        if 'generate' in s.keys():\n",
    "            print(s['generate']['generation'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip uninstall weaviate weaviate-client -y\n",
    "# !pip install weaviate-client==3.25.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO0wdjzDG2GkXJJYZQWBRMj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
